\documentclass{article}  
\author{Roger Zou and Julia Ni}
\date{\today}  
\title{Alignment of Electron Microscope Sections}  
 
\usepackage[margin=1.0in]{geometry}
\usepackage{amsthm,amssymb,amsfonts}
\usepackage[fleqn]{amsmath}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{enumitem}
\usetikzlibrary{arrows,positioning,automata, shapes}
\usepackage{graphicx}
\graphicspath{ {imgs/} }
\usepackage{wrapfig}
\usepackage[]{algorithm2e}

\begin{document}  
\maketitle

\section{Overview}
Scalable alignment of 3D electron microscope (EM) sections with CAJAL-3D API integration. Its use is domain-specific; namely, the program corrects for translations and rotations within a cube of EM image slices of the brain. See the README for details about the specific functions available to the user. The program primarily uses cross-correlation to determine rotation and translation parameters which maximize the correlation between pairwise images. This report provides details on such alignment methods, and extends the aforementioned core technique to provide scalable global alignment of terobytes of EM image data.

\subsection{Computational Complexity}
\textbf{Time complexity:} $O(nlogn)$, where $n$ is the number of voxels due to the cross-correlation procedures with the Fast Fourier Transform (FFT). The size of $n$ can be greatly reduced by picking a lower resolution image either directly from the API or by downsampling. Thus cross-correlation is the bottleneck of the alignment pipeline.

\section{Pairwise Alignment}
Before an entire 3D image cube can be globally aligned, we must compute alignment between pairs of images.
\\
The procedure uses 2D cross-correlation to identify the degree of rotation and magnitude of translations necessary for a pair of images. See section 2.1 for specifics. 
\\
One important step in the cross-correlation process is to identify peaks in the cross-correlation that correspond to the translations necessary to align the two images. Difficulties arise when the 'correct' peak is NOT the maximum value; simply picking the maximum value would result in completely inaccurate parameters. Our solution is to use a trained support vector machine (SVM) classifier to determine the correct peak among potential candidate peaks. See section 2.2 for specifics. 
\\
Once the rotation and translation transformations are identified, the rotation angle $\theta$ necessary to align two images is refined by varying $\theta$ by $\pm \epsilon$ for small increments of $\epsilon$. The $\theta \pm \epsilon$ that minimizes the error function (MSE) between two images is the improved rotation angle parameter. See section 2.3 for specifics.
\\
Finally, to incorporate alignment data from images outside adjacent image pairs $I_i, I_{i+1}$ to refine the alignment transformations, we compute the alignment transformation parameters for $I_{i-1}, I_{i+1}$ and $I_i, I_{i+2}$. We then obtain multiple estimates of the transformations between $I_i, I_{i+1}$ using the additional transformation parameter and pick the best one. See section 2.4 for specifics.

\subsection{Alignment Procedure with Correlation}
Iterate through image stack, and apply the procedures described below to each adjacent pair of images.
\begin{enumerate}
\item Apply median filtering and histogram equalization to both images.
\item Apply hamming window to both images. 
\item Take the Discrete Fourier Transform (DFT), apply a high-pass filter, and then resample the images in log-polar coordinates. 
\item Correlate the two images to find the best rho (scaling) and theta (rotation angle) parameters by max picking.
\item Rotate the image appropriately, then correlate the two (rotated) images to find the best translation transformation parameters.
\item Use an SVM to identify the peak in the cross-correlation of the image pair that accurately corresponds to the actual translation parameters.
\item Save the pairwise affine transformation matrix. 
\end{enumerate}The time complexity is $O(nlogn)$, where $n$ is the the number of voxels. This upper bound arises due to the Fast Fourier Transform (FFT) computations. The space complexity is linear to the number of voxels. One optimization step is scaling the image stack before determining image transforms. Scaling between 0.5 and 1 does not appear to have much of an effect on alignment quality (from 1024x1024 original). In practice, this method greatly improves the running time. \\

\subsection{Peak Identification in the Cross-Correlation of Two Images}
The cross-correlation of two images is an image. The intensity at the $i,j$ coordinate from the center of the image is the correlation between the two images with one image shifted by $i,j$. At a shift where the two images are aligned, the intensity is much greater. However, it is not always the case that the correct translation parameter is the maximum, or that there is even a correct translation parameter at all. Thus, simple max picking is not sufficient. Our best method uses a Support Vector Machine (SVM) to classify potential peaks as true peaks or non-peaks. The feature space characterizes shape features of the peak because the peak has a distinctive shape. The SVM method is efficient, accurate, and scalable. 

\subsubsection{SVM Training}
Taking a stack of aligned images, perform the following procedure:
\begin{enumerate}
\item Randomly sample from each image pair a cropped image with specified minimum size.
\item Generate image from the cross-correlation of the image pair; the image with its associated max value is labeled in the true (aligned) class.
\item Apply a random noticeable rotation and translation to one of the images. Generate image from the cross-correlation of the image pair (now rotated); this image with its associated max value is labeled in the false (unaligned) class. 
\item Repeat 1,2 as necessary.
\item Generate a 1x5 feature vector for each test sample. Use these feature vectors to train SVM with a linear kernel. 
\end{enumerate}
Once the SVM is trained, it is saved; in the alignment pipeline, the default config settings automatically load this trained SVM for peak classification.

\subsubsection{Peak Identification with SVM}
\begin{enumerate}
\item Divide image $I$, the cross correlation of two images, into nine equal sections $I_1...I_9$.
\item Find the coordinates of the maximum value in each section, $(x_1,y_1, I(x_1,y_1))...(x_9,y_9, I(x_9,y_9))$, where $I(x,y)$ is the intensity value at coordinate $x,y$. 
\item Sort the coordinates by order of decreasing intensity.
\item Classify $I$ with point $(x_i, y_i)$ for $i=1...9$ until a true classification arises.
The first true classification is determined to be the actual peak with specific translation parameters. If no true classification is obtained, then the image is deemed to be unaligned. 
\end{enumerate}
The delineated method works the best compared to other methods, briefly described below.\\

\textbf{Other Attempted Methods}
\begin{enumerate}
\item Identify maximum values. However, this will not always work if the maximum value is not the actual peak as described previously. 
\item Correlate the cross-correlation of two images with a normal distribution. In theory this should accentuate the true peaks since it takes into account the shape of all possible peaks, but this won't always produce the desired results because artificial peaks may be introduced. Another problem is that this method is quite slow because it has to perform more cross-correlation procedures. 
\end{enumerate}

\subsection{Tune Transformation Parameters}
Attempts to improve the estimate of the rotation angle by minimizing an error function. While three different error metrics are available to determine 'accuracy' of alignment (peak-signal-to-noise ratio (PSNR), mean-squared error (MSE), and mean pixel intensity difference.), MSE is usually used. This step is important because log-polar sampling to determine the best rotation parameter is discretized so the 'best' rotation angle may not be available at the cross-correlation step. Here, the algorithm iterates over a small range of rotation and translation parameters using the original estimated parameters as its basis, and then chooses the rotation angle and translation shift that minimize the error function.

\subsection{Error Minimization}
The final (optional) step right now is to use image data outside a specific image pair to align the image pair. This works well by correcting alignments when one pairwise alignment fails for some reason. The specifics of the algorithm are detailed below. The algorithm improves pairwise transformation parameters by computing pairwise transformations from other adjacent images. Different estimates of the transformation parameters between an image pair are computed from solving a linear programming problem. The transformation parameter that minimizes a particular error function is selected.
	\begin{algorithm}
	\DontPrintSemicolon
	\textbf{Transform Update/Error Minimization}\\
	\KwIn{Four adjacent images:$M1,M2,M3,M4$, and table of pairwise transformations and associated error.}
	\KwOut{$T_{2,3}^{*}$, the transformation matrix to align $(M2, M3)$ that minimizes error function $e_{2,3}^{*}$}
	Let $T_{2,3}$ be the transformation matrix to align $M2, M3$, and $e_{2,3}$ be the associated error.\\
	1) Compute $T_{1,3}$ and $T_{2,4}$.\\
	2) We wish to find $T_{2,3}^{*}$ that minimizes $e_{2,3}^{*}$ by using $T_{1,3},T_{2,4}$.\\
	3) Let $T_{2,3}^{*} = [T_{1,3}^{-1}*T_{1,2}]x_1 + [T_{2,4}*T_{3,4}^{-1}]x_2 + [T_{2,3}]x_3 + [I]x_4$\\
	Note that $T_{1,3}^{-1}*T_{1,2}$ and $T_{2,4}*T_{3,4}^{-1}$ are two other estimates of $T_{2,3}$.\\
	Let $e_{1,2,3}$ be the error from aligning with $T_{1,3}^{-1}*T_{1,2}$, $e_{2,3,4}$ be the error from aligning with $T_{2,4}*T_{3,4}^{-1}$.\\
	Let $e_i$ be the error from not aligning (using identity matrix $I$) \\
 	4) Pose the following equation and constraints:
	\begin{align*}
	 \text{minimize: } z &= e_{1,2,3}x_1 +  e_{2,3,4}x_2 + e_{2,3}x_3 + e_ix_4\\
	 \text{constraints: }&x_1 + x_2 + x_3 + x_4 = 1\\
	 &x_1, x_2, x_3, x_4 \ge 0
	\end{align*}
	5) Solve to find $x_1, x_2, x_3, x_4$.\\
	6) \textbf{return} $T_{2,3}^{*}$ by plugging $x_1, x_2, x_3, x_4$ into $[T_{1,3}^{-1}*T_{1,2}]x_1 + [T_{2,4}*T_{3,4}^{-1}]x_2 + [T_{2,3}]x_3 + [I]x_4$
	\end{algorithm}

\section{Global Stack Alignment}
Using the pairwise transformation parameters determined previously, the next step is to compute global transformation parameters (e.g. transformations for each image in cube relative to one image). The algorithm is as follows:
	\begin{algorithm}
	\DontPrintSemicolon
	\KwIn{Unaligned Image cube $I$, pairwise transformation parameters $T$}
	\KwOut{Aligned Image Cube $I_{aligned}$}
	Let the global transformation parameters for an image $i$ be $T_{global}(i,:)$ and for $T_{global}$ to be equal to the zero matrix initially, with $T_{global}(1,:)$ = $[0, 0, 0]$ always true. \\
	\begin{enumerate}
	\item Subsequent image in the cube, $I_{2}$, has $T_{global}(2,:)$ = $T_{global}(1,:)$. \\
	\item New rotation angle $T_{global}(2,3)$ is given by $T_{global}(2,3)$ + $T_{1,2}(3)$, the sum of the cumulative rotations the previous image underwent and the pairwise rotation between those two images. \\
	\item New translation parameters $T_{param,1,2}$ determined by the product of the previous rotation matrix (using $T_{global}(1,3)$) with the pairwise translation matrix (using $T_{1,2}(1:2)$). \\
	\item If $x$-translation $T_{param,1,2}(1)$ \textgreater\ $0$, then $T_{global}(2,1)$ = $T_{global}(2,1)$ + $T_{param,1,2}(1)$; if $x$-translation $T_{param,1,2}(1)$ \textless\ $0$, then $T_{global}(2,1)$ = $T_{global}(2,1)$ + $|T_{param,1,2}(1)|$. The same holds for $y$-translation. \\
	\item Repeat Steps 1-5 for remaining pairwise, unaligned images, updating and then using the updated $T_{global}$, to get $I_{aligned}$. 
	\end{enumerate}
	\end{algorithm}

\section{Attempted, But Ultimately Unused, Methods}
\subsection{RANSAC}
Originally implemented to detect folds in the images and subsequently help split images into pieces to aid in alignment process. We have not encountered any thus far, so this functionality was never included in our pipeline.
\subsection{SURF Feature Matching}
Originally used to correct large rotations before aligning with cross-correlation. Eventually discarded because images are often too different to match features well. Downsampling did not help much for some images.\\
\begin{enumerate}
\item Use Gaussian blur and then downsample (via MATLAB's $imresize$). Also tried median filtering and anisotropic diffusion without characterizable improvement.
\item Detect SURF features (experimented with most feature detectors in MATLAB, but SURF appeared to work the best). 
\item Match the features and estimate the rotation parameter.
\item Save the rotation angle and apply the rotation to image for input to 2D cross-correlation step.
\end{enumerate}
\textbf{Problems:} Effectiveness decreases for more dissimilar images. SURF also struggles to identify features or makes spurious matches when there are noise and blemishes in the images. This step does not seem to lead to drastic improvements. \\
\subsection{Error Metric with Superpixels and Earth Mover's Distance}
A novel idea to use Earth Mover's Distance on superpixels as an alignment error metric between two images $P$ and $Q$.
\begin{enumerate}
\item Compute superpixels of each image using the SLIC algorithm.
\item Identify the coordinates of each cluster center and its associated weight as the sum over all pixels in the cluster $\times$ mean image intensity $/$ 255. 
\item Let $D$ be the $n \times n$ distance matrix, where $n$ is the number of cluster centers. Let $F$ be the $n \times n$ matrix of total flow. For both $D$ and $F$, the $i,j$ entry indicates the distance or flow, respectively, from the $ith$ cluster center to the $jth$ cluster center. 
\item Solve transportation problem as a linear program with constraints as detailed in Rubner et al. \cite{rubner00} to find all entries in $F$.
\item Find the EMD, defined as $EMD(P,Q) = \frac{ \sum_{i=1}^m \sum_{j=1}^n d_{ij} f_{ij} } { \sum_{i=1}^m \sum_{j=1}^n f_{ij} }$.
\end{enumerate}

\begin{thebibliography}{9}

\bibitem{lamport94}
	B. Srinivasa Reddy and B. N. Chatterji,
	\emph{An FFT-Based Technique for Translation, Rotation, and Scale-Invariant Image Registration}.
	IIEEE Transactions on Image Processing Vol. 5, No. 8, 1996
\bibitem{rubner00}
	Yossi Rubner, Carlo Tomasi, Leonidas J. Guibas,
	\emph{The Earth Mover's Distance as a Metric for Image Retrieval}.
	International Journal of Computer Vision 40(2), 99-121, 2000
\bibitem{wang13}
	Peng Wang, Gang Zeng, Rui Gan, Jingdong Wang, Hongbin Zha,
	\emph{Structure-Sensitive Superpixels via Geodesic Distance}.
	International Journal of Computer Vision, 103:1-21, 2013

\end{thebibliography}

\end{document}
